{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (2.0.2)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.12.1-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from h5py) (2.0.2)\n",
      "Downloading h5py-3.12.1-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.1/3.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 9.7 MB/s eta 0:00:00\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('dataset/trainset.hdf5', \"r\")\n",
    "    X_train = np.array(train_dataset[\"X_train\"][:]) # your train set features\n",
    "    y_train = np.array(train_dataset[\"Y_train\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('dataset/testset.hdf5', \"r\")\n",
    "    X_test = np.array(test_dataset[\"X_test\"][:]) # your train set features\n",
    "    y_test = np.array(test_dataset[\"Y_test\"][:]) # your train set labels\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1000, 4096)\n",
      "y_train shape: (1000, 1)\n",
      "x_test shape: (200, 4096)\n",
      "y_test shape: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Normalize image vectors\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Normalize labels pas necessaire car les labels sont déjà normalisés\n",
    "# flatten les image\n",
    "x_train = x_train.reshape(x_train.shape[0], 4096) # 64x64 = 4096 ou -1 pour laisser numpy calculer\n",
    "x_test = x_test.reshape(x_test.shape[0], 4096)\n",
    "\n",
    "# afficher les dimensions\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w shape: (4096, 1)\n",
      "b shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "# generer random weights et bias\n",
    "w = np.random.randn(4096, 1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# afficher les dimensions\n",
    "print(\"w shape:\", w.shape)\n",
    "print(\"b shape:\", b.shape)\n",
    "\n",
    "# initialiser w et b\n",
    "def initialize_with_zeros(nb_weights=4096):\n",
    "    w = np.random.randn(nb_weights, 1)\n",
    "    b = np.random.randn(1)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.4.1-cp39-cp39-win_amd64.whl (199.3 MB)\n",
      "   ---------------------------------------- 0.0/199.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/199.3 MB 4.2 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 1.8/199.3 MB 4.8 MB/s eta 0:00:42\n",
      "    --------------------------------------- 3.1/199.3 MB 4.9 MB/s eta 0:00:41\n",
      "    --------------------------------------- 4.2/199.3 MB 4.8 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 5.2/199.3 MB 4.8 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 6.6/199.3 MB 4.9 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 7.6/199.3 MB 4.8 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 8.9/199.3 MB 4.9 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 10.0/199.3 MB 4.9 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 11.0/199.3 MB 4.9 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 12.3/199.3 MB 4.9 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 13.1/199.3 MB 4.9 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 13.6/199.3 MB 4.6 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 14.7/199.3 MB 4.5 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 15.7/199.3 MB 4.6 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 16.8/199.3 MB 4.6 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 17.8/199.3 MB 4.6 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 19.1/199.3 MB 4.6 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 20.2/199.3 MB 4.7 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 21.5/199.3 MB 4.7 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 22.5/199.3 MB 4.7 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 23.6/199.3 MB 4.7 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 23.6/199.3 MB 4.7 MB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 25.2/199.3 MB 4.6 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 27.0/199.3 MB 4.7 MB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 28.3/199.3 MB 4.7 MB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 29.6/199.3 MB 4.8 MB/s eta 0:00:36\n",
      "   ------ --------------------------------- 30.7/199.3 MB 4.8 MB/s eta 0:00:36\n",
      "   ------ --------------------------------- 32.0/199.3 MB 4.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 33.3/199.3 MB 4.8 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 34.6/199.3 MB 4.9 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 35.7/199.3 MB 4.9 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 37.0/199.3 MB 4.9 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 38.3/199.3 MB 4.9 MB/s eta 0:00:33\n",
      "   ------- -------------------------------- 39.6/199.3 MB 4.9 MB/s eta 0:00:33\n",
      "   -------- ------------------------------- 41.2/199.3 MB 5.0 MB/s eta 0:00:32\n",
      "   -------- ------------------------------- 42.2/199.3 MB 5.0 MB/s eta 0:00:32\n",
      "   -------- ------------------------------- 43.5/199.3 MB 5.0 MB/s eta 0:00:32\n",
      "   -------- ------------------------------- 44.6/199.3 MB 5.0 MB/s eta 0:00:32\n",
      "   --------- ------------------------------ 45.9/199.3 MB 5.0 MB/s eta 0:00:31\n",
      "   --------- ------------------------------ 47.2/199.3 MB 5.0 MB/s eta 0:00:31\n",
      "   --------- ------------------------------ 48.5/199.3 MB 5.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 49.8/199.3 MB 5.0 MB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 51.1/199.3 MB 5.1 MB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 52.2/199.3 MB 5.1 MB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 53.5/199.3 MB 5.1 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 54.8/199.3 MB 5.1 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 55.8/199.3 MB 5.1 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 57.1/199.3 MB 5.1 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 58.2/199.3 MB 5.1 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 59.2/199.3 MB 5.1 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 60.3/199.3 MB 5.1 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 61.1/199.3 MB 5.0 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 62.1/199.3 MB 5.0 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 63.2/199.3 MB 5.0 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 64.2/199.3 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------- -------------------------- 65.3/199.3 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------- -------------------------- 66.1/199.3 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------- -------------------------- 67.4/199.3 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------- -------------------------- 68.7/199.3 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------- -------------------------- 69.7/199.3 MB 5.0 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 71.0/199.3 MB 5.0 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 72.1/199.3 MB 5.0 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 72.9/199.3 MB 5.0 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 73.4/199.3 MB 5.0 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 73.4/199.3 MB 5.0 MB/s eta 0:00:26\n",
      "   -------------- ------------------------- 74.4/199.3 MB 4.9 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 75.8/199.3 MB 4.9 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 77.1/199.3 MB 4.9 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 78.4/199.3 MB 4.9 MB/s eta 0:00:25\n",
      "   --------------- ------------------------ 79.7/199.3 MB 4.9 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 80.7/199.3 MB 4.9 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 82.1/199.3 MB 4.9 MB/s eta 0:00:24\n",
      "   ---------------- ----------------------- 83.4/199.3 MB 4.9 MB/s eta 0:00:24\n",
      "   ---------------- ----------------------- 84.7/199.3 MB 4.9 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 86.0/199.3 MB 5.0 MB/s eta 0:00:23\n",
      "   ----------------- ---------------------- 87.3/199.3 MB 5.0 MB/s eta 0:00:23\n",
      "   ----------------- ---------------------- 88.6/199.3 MB 5.0 MB/s eta 0:00:23\n",
      "   ----------------- ---------------------- 89.7/199.3 MB 5.0 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 91.0/199.3 MB 5.0 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 92.0/199.3 MB 5.0 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 93.3/199.3 MB 5.0 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 94.6/199.3 MB 5.0 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 95.4/199.3 MB 5.0 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 96.2/199.3 MB 5.0 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 97.0/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 97.8/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 98.3/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 99.4/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 100.4/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 101.2/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 101.7/199.3 MB 4.9 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 102.5/199.3 MB 4.8 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 103.0/199.3 MB 4.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 104.1/199.3 MB 4.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 104.6/199.3 MB 4.8 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 105.4/199.3 MB 4.8 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 106.4/199.3 MB 4.8 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 106.4/199.3 MB 4.8 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 108.3/199.3 MB 4.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 109.3/199.3 MB 4.7 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 110.1/199.3 MB 4.7 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 110.6/199.3 MB 4.7 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 111.4/199.3 MB 4.7 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 112.2/199.3 MB 4.7 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 113.0/199.3 MB 4.6 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 113.5/199.3 MB 4.6 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 114.0/199.3 MB 4.6 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 114.8/199.3 MB 4.6 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 115.6/199.3 MB 4.6 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 116.1/199.3 MB 4.6 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 117.4/199.3 MB 4.6 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 118.8/199.3 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 119.8/199.3 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 121.1/199.3 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 122.4/199.3 MB 4.6 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 123.7/199.3 MB 4.6 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 124.8/199.3 MB 4.6 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 126.1/199.3 MB 4.6 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 127.4/199.3 MB 4.6 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 128.5/199.3 MB 4.6 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 129.8/199.3 MB 4.6 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 131.1/199.3 MB 4.6 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 132.1/199.3 MB 4.6 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 133.4/199.3 MB 4.6 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 134.5/199.3 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 135.8/199.3 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 136.8/199.3 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 138.1/199.3 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 139.5/199.3 MB 4.7 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 140.8/199.3 MB 4.7 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 142.1/199.3 MB 4.7 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 143.1/199.3 MB 4.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 144.4/199.3 MB 4.7 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 146.0/199.3 MB 4.7 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 147.3/199.3 MB 4.7 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 148.4/199.3 MB 4.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 149.9/199.3 MB 4.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 151.3/199.3 MB 4.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 152.3/199.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 153.9/199.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 155.2/199.3 MB 4.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 156.5/199.3 MB 4.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 157.8/199.3 MB 4.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 159.1/199.3 MB 4.8 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 159.9/199.3 MB 4.8 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 161.0/199.3 MB 4.8 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 162.3/199.3 MB 4.8 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 163.6/199.3 MB 4.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 164.9/199.3 MB 4.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 166.2/199.3 MB 4.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 167.2/199.3 MB 4.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 168.6/199.3 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 169.9/199.3 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 171.2/199.3 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 172.2/199.3 MB 4.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 173.5/199.3 MB 4.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 174.6/199.3 MB 4.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 175.9/199.3 MB 4.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 176.9/199.3 MB 4.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 177.7/199.3 MB 4.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 178.8/199.3 MB 4.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 179.8/199.3 MB 4.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 181.1/199.3 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 182.2/199.3 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 183.5/199.3 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 184.8/199.3 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 185.9/199.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 186.9/199.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 188.0/199.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 188.7/199.3 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 189.8/199.3 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 190.8/199.3 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 191.9/199.3 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 192.9/199.3 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 194.2/199.3 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  195.3/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  196.3/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  197.7/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  198.7/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.2/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.2/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.2/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.2/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.2/199.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.3/199.3 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 4.3 MB/s eta 0:00:00\n",
      "Using cached MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.16.1 fsspec-2024.9.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.3 torch-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp39-cp39-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\home\\documents\\github\\classify_picture_logistic-regression\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.8 MB 3.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.6/7.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.6/7.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/7.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/7.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.2 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.4/2.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-10.4.0 pyparsing-3.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model(X, w, b):\n",
    "    \"\"\"\n",
    "    fonction qui prédit la classe d'une image\n",
    "    @param X: image flatten\n",
    "    @param w: poids\n",
    "    @param b: bias\n",
    "    @return: probabilité que l'image appartienne à la classe 1\n",
    "    \"\"\"\n",
    "    z = np.dot(X, w) + b # produit matriciel et b use broadcasting pour combler b\n",
    "    a = 1 / (1 + np.exp(-z)) # sigmoid car je veux une probabilité entre 0 et 1 (deux classes : si < 0.5 alors 0 sinon 1)\n",
    "    return a\n",
    "\n",
    "def log_loss(a, y):\n",
    "    \"\"\"\n",
    "    fonction qui calcule la log loss (bceloss)\n",
    "    @param a: probabilité que l'image appartienne à la classe 1\n",
    "    @param y: vraie classe de l'image\n",
    "    @return: log loss\n",
    "    \"\"\"\n",
    "    return  nn.BCELoss(a,y)\n",
    "\n",
    "def gradient(X, A, y):\n",
    "    \"\"\"Fonction qui calcule le gradient de la fonction de cout par rapport à w et b en utilsant le principe de la chaine afin de decomposer le calcule de depart qui est derive du cout par rapport a w ou b car effectivmenet, on a pas directement w1 et b dans la fonctio nde cout donc on doit decomposer le calcule(voir explication plus bas).\n",
    "    @param X: image flatten\n",
    "    @param a: probabilité que l'image appartienne à la classe 1\n",
    "    @param y: vraie classe de l'image\n",
    "    @return: matrice des gradient de la fonction de cout par rapport à w et idem pour b\n",
    "    ca nous donne les gradient pour chaque poids et bias qu servi a mettre a jour les poids et bias plus tard\n",
    "    \"\"\"\n",
    "    # derive de la fonction de cout par rapport à w et b calculé au prealable et ca donne ca\n",
    "    # attention, si la fonction de perte/cout change ou que la fonction d activation change, il faut recalculer le gradient ou use pytorch, autograd ou keras qui le fait otut seul\n",
    "    dW = 1 / len(y) * np.dot(X.T, A - y)\n",
    "    db = 1 / len(y) * np.sum(A - y)\n",
    "    return dW, db\n",
    "\n",
    "def update(dW, db, W, b, learning_rate):\n",
    "    \"\"\"\n",
    "    fonction qui met à jour les poids et le bias\n",
    "    @param dW: gradient de la fonction de cout par rapport à w\n",
    "    @param db: gradient de la fonction de cout par rapport à b\n",
    "    @param W: poids\n",
    "    @param b: bias\n",
    "    @param learning_rate: taux d'apprentissage (pas donc la vitesse a laquelle on apprend)\n",
    "    @return: nouveaux poids et bias\n",
    "    \"\"\"\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    return (W, b)\n",
    "\n",
    "def train(X, y, learning_rate, epochs=1000):\n",
    "    \"\"\"\n",
    "    fonction qui entraine le modèle\n",
    "    @param X: images flatten\n",
    "    @param y: vraies classes des images\n",
    "    @param w: poids\n",
    "    @param b: bias\n",
    "    @param learning_rate: taux d'apprentissage\n",
    "    @param epochs: nombre d'itérations\n",
    "    @return: poids et bias entrainés\n",
    "    \"\"\"\n",
    "    # init model\n",
    "    loss = []\n",
    "    w, b = initialize_with_zeros()\n",
    "    # boucle d entrainement\n",
    "    for epoch in range(epochs):\n",
    "        # prédire la classe\n",
    "        A = model(X, w, b)\n",
    "        # calculer la fonction de cout\n",
    "        loss = log_loss(A, y)\n",
    "        # calculer le gradient\n",
    "        dW, db = gradient(X, A, y)\n",
    "        # mettre à jour les poids\n",
    "        w, b = update(dW, db, w, b, learning_rate)\n",
    "        # afficher la loss et epoch\n",
    "        print(f\"Epoch {epoch} - loss: {loss}\")\n",
    "        # save loss\n",
    "        loss.append((loss, epoch))\n",
    "    return w, b, loss\n",
    "\n",
    "# --------------------------------entrainer le modèle----------------------------\n",
    "w, b, loss = train(x_train, y_train, 0.01, 1000)\n",
    "\n",
    "# afficher evolution loss\n",
    "# Créer la figure et tracer la courbe\n",
    "plt.figure(figsize=(8, 5))  # Taille de la figure\n",
    "plt.plot(np.array(loss)[:, :1], np.array(loss)[:, 1:2], label='y = x^2', linestyle='-', marker='o')  # Tracer y en fonction de x\n",
    "\n",
    "# Ajouter un titre et des labels\n",
    "plt.title(\"Courbe de y = x^2\")\n",
    "plt.xlabel(\"Valeur de la perte\")\n",
    "plt.ylabel(\"Valeur de l'epoch\")\n",
    "\n",
    "# Ajouter une légende\n",
    "plt.legend()\n",
    "\n",
    "# Afficher la grille pour faciliter la lecture du graphique\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------tester le modèle----------------------------\n",
    "def predcit(X, w, b, y):\n",
    "    \"\"\"\n",
    "    fonction qui prédit la classe d'une image\n",
    "    @param X: image flatten\n",
    "    @param w: poids\n",
    "    @param b: bias\n",
    "    @return: classe prédite\n",
    "    \"\"\"\n",
    "    A = model(X, w, b)\n",
    "    # calculer la fonction de cout\n",
    "    loss = log_loss(A, y)\n",
    "    return loss\n",
    "    \n",
    "def predict_one_value(x, w, b):\n",
    "    \"\"\"\n",
    "    fonction qui prédit la classe d'une image\n",
    "    @param x: image flatten\n",
    "    @param w: poids\n",
    "    @param b: bias\n",
    "    @return: classe prédite\n",
    "    \"\"\"\n",
    "    # prédire la classe\n",
    "    A = model(x, w, b)\n",
    "    # si la probabilité est supérieure à 0.5, on prédit 1 sinon 0\n",
    "    return A > 0.5\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [3]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "list_ = [(1, 2), (3, 4), (5, 6)]\n",
    "array_ = np.array(list_)\n",
    "list_1 = array_[:, :1]\n",
    "print(list_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
